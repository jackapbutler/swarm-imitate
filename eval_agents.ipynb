{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from swarmi.game import (\n",
    "    evaluate,\n",
    "    agent_init,\n",
    "    get_distance_reward,\n",
    "    represent_poolers\n",
    ")\n",
    "from swarmi.operators import (\n",
    "    prodop,\n",
    "    NeuralNetOp,\n",
    "    average,\n",
    "    lambda_update,\n",
    "    weighted_avg\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAME\n",
    "## one hypothesis only, agents care about P(H=true)\n",
    "N_REPEATS = 500\n",
    "NUM_ITERS = 50 # 20\n",
    "N_AGENTS = 100\n",
    "MAX_POOLERS = 20 # 10\n",
    "MIN_POOLERS = 10\n",
    "N_POOLS = 1\n",
    "AGENT_FUNC = lambda: N_AGENTS\n",
    "INITIALISATION = functools.partial(\n",
    "    agent_init, \n",
    "    belief_init_func=lambda: random.uniform(0,1), \n",
    "    evidence_init_func=lambda: random.uniform(0,.5), \n",
    "    noise_init_func=lambda: random.uniform(.5,1),\n",
    "    pooling_init_func=lambda: random.uniform(0,1),\n",
    "    communication_init_func=lambda: random.uniform(0,.1),\n",
    "    zealot_init_func=lambda: random.uniform(0,.5),\n",
    ")\n",
    "REWARD_FUNC = functools.partial(get_distance_reward, sign=1, exp=1)\n",
    "N_TRUTH_FLIPS = 0 # 5\n",
    "UPDATE_FUNC = None # functools.partial(lambda_update, _lambda_new=0.90)\n",
    "PREPROCESS_FUNC = None # lambda x_features: x_features[2] \n",
    "\n",
    "trial_name = 'bigbatch'\n",
    "NN_POLICIES = {\n",
    "    # tests on big batch\n",
    "    'RL BigBatch 60k steps': NeuralNetOp(f'./models/{trial_name}_62500.pt'),\n",
    "    'RL BigBatch 125k steps': NeuralNetOp(f'./models/{trial_name}_125000.pt'),\n",
    "    'RL BigBatch 600k steps': NeuralNetOp(f'./models/{trial_name}_done.pt'),\n",
    "\n",
    "    # official paper models    \n",
    "    # 'RL 2M': NeuralNetOp(f'./models/rl_varied_n_avgagg_6D_done.pt'),\n",
    "    'Imitation NN - Avg': NeuralNetOp(\"./models/imitated_avg_10k_50e_150width_20maxagents_6D_normalised_avg_agg.pt\"),\n",
    "    # 'Imitation NN - NormLogOp': NeuralNetOp(\"./models/imitated_normprodop_10k_50e_150width_20maxagents_6D_normalised_avg_agg.pt\")\n",
    "\n",
    "    # tests on initialisation\n",
    "    **{f'RL - {x} init': NeuralNetOp(f'./models/{x}_0_done.pt') for x in ['fromrandom', 'fromavg', 'fromsumaggavg', 'fromnormlogop']}\n",
    "    \n",
    "}\n",
    "ALL_POLICIES = {'Avg': average, **NN_POLICIES} # only used for some plots below\n",
    "BASELINE_OPS = {'Avg': weighted_avg, 'LogOp': prodop}\n",
    "PREPROC_FUNCS = {'1': lambda x: 1, '$w_{truth}$': None}\n",
    "_LAMBDAS = [0.95, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_scores = {}\n",
    "for param_val in [x for x in range(MIN_POOLERS-5,MAX_POOLERS+5+1)]:\n",
    "    print(param_val)\n",
    "    # get eval results\n",
    "    common_args = (\n",
    "        N_REPEATS,\n",
    "        AGENT_FUNC,\n",
    "        NUM_ITERS,\n",
    "        int(param_val),\n",
    "        int(param_val),\n",
    "        N_POOLS,\n",
    "        INITIALISATION,\n",
    "        REWARD_FUNC,\n",
    "        N_TRUTH_FLIPS\n",
    "    )\n",
    "    scores, belief_history = evaluate(\n",
    "        NN_POLICIES,\n",
    "        *common_args,\n",
    "        UPDATE_FUNC,\n",
    "        PREPROCESS_FUNC,\n",
    "        using_nn_pooling=True\n",
    "    )\n",
    "    for op_name, operator in BASELINE_OPS.items():\n",
    "        for normalised in [True, False]:\n",
    "            for preproc_name, preproc in PREPROC_FUNCS.items():\n",
    "                for _lambda in _LAMBDAS:\n",
    "                    if operator == weighted_avg and not normalised:\n",
    "                        continue\n",
    "                    _operator = functools.partial(prodop, normalise_w=True) if operator == prodop and normalised else operator\n",
    "                    scores_v, belief_history_v = evaluate(\n",
    "                        {\n",
    "                            f'{op_name}, normalised={normalised}, $w_i$={preproc_name} $\\lambda$={_lambda}': _operator\n",
    "                        },\n",
    "                        *common_args,\n",
    "                        functools.partial(lambda_update, _lambda_new=_lambda),\n",
    "                        preproc,\n",
    "                        using_nn_pooling=False\n",
    "                    )\n",
    "                    scores.update(scores_v)\n",
    "                    belief_history.update(belief_history_v)\n",
    "\n",
    "    # get metrics out\n",
    "    grid_scores[param_val] = {}\n",
    "    for policy, grid_values in scores.items():\n",
    "        avg_scores = []\n",
    "        for values in grid_values:\n",
    "            # gets average across time\n",
    "            avg_scores.append(sum(values)/len(values))\n",
    "\n",
    "        # mean across trials (for each param)\n",
    "        mean_value = np.mean(avg_scores)\n",
    "        std_value = np.std(avg_scores, ddof=1)  # Use ddof=1 for sample standard deviation\n",
    "        confidence_interval = 1.96 * (std_value / np.sqrt(len(avg_scores)))\n",
    "\n",
    "        grid_scores[param_val][policy] = {'mean': float(mean_value), 'ci': float(confidence_interval), 'samples': avg_scores}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./grid_results_{N_REPEATS}.json', 'w') as f:\n",
    "    json.dump(grid_scores, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./grid_results_{N_REPEATS}.json', 'r') as f:\n",
    "    grid_scores = json.load(f)\n",
    "    grid_scores = {int(k) : v for k, v in grid_scores.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_label(x: str):\n",
    "    if 'RL' in x:\n",
    "        return 'Reinforcement Learned'\n",
    "    elif 'NN' in x:\n",
    "        if 'NormLogOp' in x:\n",
    "            return 'Supervised Learned (NormLogOp)'\n",
    "        elif 'Avg' in x:\n",
    "            return 'Supervised Learned'\n",
    "    else:\n",
    "        return x.replace(\"Avg, normalised=True,\", \"Weighted Average\").replace('LogOp, normalised=True,', 'NormLogOp').replace(', normalised=False,', '').replace('$w_{truth}$', '$\\omega_t$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = []\n",
    "ys = {p: [] for p in grid_scores[list(grid_scores.keys())[0]].keys()}\n",
    "ys_err = {p: [] for p in grid_scores[list(grid_scores.keys())[0]].keys()}\n",
    "\n",
    "for param_val, policy_vals in grid_scores.items():\n",
    "    xs.append(param_val)\n",
    "    for policy in ys.keys():\n",
    "        ys[policy].append(policy_vals[policy]['mean'])    \n",
    "        ys_err[policy].append(policy_vals[policy]['ci'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in ys:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(22, 9))\n",
    "markers = {\n",
    "    'R': 'o',\n",
    "    'N': 's',\n",
    "    'W': 'D',\n",
    "    'L': 'X',\n",
    "    'S': 'o'\n",
    "}\n",
    "for p, vals in ys.items():\n",
    "    if p in [\n",
    "        *[x for x in ys.keys() if 'NN' in x or 'RL' in x],\n",
    "        'LogOp, normalised=True, $w_i$=1 $\\lambda$=0.95', \n",
    "        'LogOp, normalised=False, $w_i$=1 $\\lambda$=0.95', \n",
    "        'Avg, normalised=True, $w_i$=$w_{truth}$ $\\lambda$=1',\n",
    "        'LogOp, normalised=False, $w_i$=1 $\\lambda$=1', \n",
    "    ]:\n",
    "        if 'NN - NormLogOp' in p:\n",
    "            continue\n",
    "        l = convert_label(p)\n",
    "        plt.errorbar(xs, vals, ys_err[p], label=l, marker=markers[l[0]], markersize=10, capsize=6)\n",
    "\n",
    "# Add vertical lines with arrows to indicate the training pool size\n",
    "plt.axvline(x=20, color='r', linestyle='--')\n",
    "plt.axvline(x=10, color='r', linestyle='--')\n",
    "# Add a horizontal arrow between the two vertical lines\n",
    "plt.annotate('', xy=(20, 0.18), xytext=(10, 0.18),\n",
    "             arrowprops=dict(arrowstyle='<->', color='red'))\n",
    "plt.text(15, 0.185, 'Training Interval', horizontalalignment='center', fontsize=16, color='red')\n",
    "\n",
    "\n",
    "# Increase the size of the axis ticks and labels\n",
    "plt.xticks(ticks=xs, fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "\n",
    "legend = plt.legend(ncol=3, fontsize=12.1, fancybox=False, edgecolor=\"black\")\n",
    "legend.get_frame().set_linewidth(0.5)\n",
    "plt.xlabel('$N_{pool.size}$', fontsize=18)\n",
    "plt.ylabel('Avg. MAE Â± 95% CI', fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_in_training_dist = {k:v for k,v in grid_scores.items() if int(k) > 9 and int(k) < 21}\n",
    "first_key = list(results_in_training_dist.keys())[0]\n",
    "samples = {p: [] for p in results_in_training_dist[first_key].keys()}\n",
    "\n",
    "for param_val, policy_vals in results_in_training_dist.items():\n",
    "    for policy in samples.keys():\n",
    "        samples[policy].extend(policy_vals[policy]['samples'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(samples)\n",
    "df.columns = [convert_label(x) for x in df.columns]\n",
    "for k in df.columns:\n",
    "    if 'NN - NormLogOp' in k:\n",
    "        continue\n",
    "    std_dev = df[k].std(ddof=1)\n",
    "    ci_interval = 1.96 * (std_dev / np.sqrt(len(df[k])))\n",
    "    print(f\"{k}: {df[k].mean():.3f} +/- {ci_interval:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = agent_init(\n",
    "    MIN_POOLERS,\n",
    "    belief_init_func=lambda: random.uniform(0,1),\n",
    "    evidence_init_func=lambda: .25,\n",
    "    noise_init_func=lambda: 0.75,\n",
    "    pooling_init_func=lambda: 0.5,\n",
    "    communication_init_func=lambda: 0.05,\n",
    "    zealot_init_func=lambda: 0.25,\n",
    ")\n",
    "\n",
    "corrected = {}\n",
    "for a, rep in agents.items():\n",
    "    if a < 5:\n",
    "        corrected[a] = (1, *rep[1:])\n",
    "    else:\n",
    "        corrected[a] = (0, *rep[1:])\n",
    "corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reliable = (0.5, 1, 0.01, 0.001, 0.25)\n",
    "unreliable = (0.5, 0.5, .01, 0.001, 0.25)\n",
    "avg_reliable = (0.25, 0.5, 0.5, 0.001, 0.25)\n",
    "const = avg_reliable\n",
    "\n",
    "preproc = None\n",
    "logop = functools.partial(prodop, normalise_w=False)\n",
    "_lambda = 0.95\n",
    "\n",
    "def kl_divergence(p, q):\n",
    "    return sum(p[i] * np.log(p[i]/q[i]) for i in range(len(p)))\n",
    "\n",
    "def entropy(p):\n",
    "    return -sum(p[i] * np.log(p[i]) for i in range(len(p)))\n",
    "\n",
    "effects = {\n",
    "    p.replace('\\n', ' - '): {\n",
    "        'kls': [], \n",
    "        'entropy_diff': [], \n",
    "        'policy': ALL_POLICIES[p] if p in ALL_POLICIES else prodop\n",
    "    }\n",
    "    for p in ['Avg', f'LogOp $w_i=1$ $\\lambda=${_lambda}'] + list(NN_POLICIES.keys())\n",
    "}\n",
    "\n",
    "n_agents = 10\n",
    "xs = [round(x,4) for x in np.linspace(0.01,1-0.01,20)]\n",
    "for p in effects.keys():\n",
    "    for x1 in xs:\n",
    "        ys = []\n",
    "        for x2 in xs:\n",
    "            effects[p]['kls'].append(kl_divergence([x1,1-x1], [x2,1-x2]))\n",
    "            original_entropy = ((entropy([x1,1-x1]) + entropy([x2,1-x2]))/2)\n",
    "            \n",
    "            beliefs = [x1 if a < 5 else x2 for a in range(n_agents)]\n",
    "            example = {i: (b, *const) for i, b in enumerate(beliefs)}\n",
    "            is_nn = 'NN' in p or 'RL' in p\n",
    "            merged_reps = represent_poolers(\n",
    "                list(example.values()), \n",
    "                preproc if 'Product' in p else None, \n",
    "                True if is_nn else False\n",
    "            )   \n",
    "            out = effects[p]['policy'](merged_reps)\n",
    "            if 'Product' in p:\n",
    "                out1 = lambda_update(x1, out, _lambda_new=_lambda)\n",
    "                out2 = lambda_update(x2, out, _lambda_new=_lambda)\n",
    "                new_ent = ((entropy([out1,1-out1]) + entropy([out2,1-out2]))/2)\n",
    "            else:\n",
    "                new_ent = entropy([out,1-out])\n",
    "            ent_diff = new_ent - original_entropy\n",
    "            effects[p]['entropy_diff'].append(ent_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = {\n",
    "    'fromrandom init': 'Random Init.',\n",
    "    'fromavg init': 'Avg. Init.',\n",
    "    'fromnormlogop init': 'NormLogOp Init.',\n",
    "}\n",
    "for policy_to_check in effects:\n",
    "    entropy_diff_array = np.array(effects[policy_to_check]['entropy_diff'])\n",
    "    plt.imshow(entropy_diff_array.reshape(20, 20)[::-1], cmap='coolwarm', extent=[0, 1, 0, 1])\n",
    "    plt.clim(-1,1)\n",
    "    plt.colorbar()\n",
    "    for k,v in names.items():\n",
    "        if k in policy_to_check:\n",
    "            policy_to_check = policy_to_check.replace(k, v)\n",
    "    # plt.title(f'$\\Delta$ Avg. Entropy: {policy_to_check}')\n",
    "    plt.xlabel('$X_{B}$')\n",
    "    plt.ylabel('$X_{A}$')\n",
    "    print(policy_to_check)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "for e in effects:\n",
    "    plt.scatter(effects[e]['kls'], effects[e]['entropy_diff'], label=e, marker='D' if 'RL' in e else 'o')\n",
    "plt.xlabel('KL Divergence (between $X_{A}$, $X_{B}$)')\n",
    "plt.ylabel('$\\Delta$ Entropy (post-pooling)')\n",
    "plt.legend()\n",
    "plt.title('KL Divergence vs Entropy Difference, homogeneous swarm, half with $X_{A}$, half with $X_{B}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variances = {}\n",
    "xs = [round(x,4) for x in np.linspace(0.01,1-0.01,20)]\n",
    "repeats = 200\n",
    "for name, policy in [\n",
    "    ['Imitation NN NormLogOp', NN_POLICIES['Imitation NN - NormLogOp']],\n",
    "    ['RL 2M', NN_POLICIES['RL 2M']], \n",
    "    ['Avg', average], \n",
    "    ['LogOp $w_i=w_{truth}$', prodop], \n",
    "]:\n",
    "    variances[name] = []\n",
    "    for x1 in xs:\n",
    "        ys = []\n",
    "        for x2 in xs:\n",
    "            samples = []\n",
    "            for repeat in range(repeats):\n",
    "                beliefs = INITIALISATION(10)\n",
    "                beliefs = [x1 if a < 5 else x2 for a in range(n_agents)]\n",
    "                example = {i: (b, *const) for i, b in enumerate(beliefs)}\n",
    "                is_nn = True if ('RL' in name or 'NN' in name) else False\n",
    "                merged_reps = represent_poolers(list(example.values()), None, is_nn)\n",
    "                out = policy(merged_reps)\n",
    "                samples.append(out)\n",
    "            stat = np.var(samples)\n",
    "            ys.append(stat)\n",
    "        variances[name].append(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = None\n",
    "for name, variance in variances.items():\n",
    "    variance_arr = np.array(variance)\n",
    "    # plt.figure(figsize=(4,4))\n",
    "    plt.imshow(variance_arr.reshape(20, 20)[::-1], cmap='coolwarm', extent=[0, 1, 0, 1])\n",
    "    plt.colorbar().set_label('$\\sigma^2$')\n",
    "    plt.title(f\"{name}\")\n",
    "    plt.xlabel('$X_{B}$')\n",
    "    plt.ylabel('$X_{A}$')\n",
    "    if cap:\n",
    "        plt.clim(1e-8,cap)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "attr_name_to_index = {\n",
    "    '$x$': 0,\n",
    "    '$w_{evidence}$': 1,\n",
    "    '$w_{truth}$': 2,\n",
    "    '$w_{pool}$': 3,\n",
    "    '$w_{noise}$': 4,\n",
    "    '$w_{doubt}$': 5\n",
    "}\n",
    "attr_vals = {a: [] for a in attr_name_to_index}\n",
    "n_agents = 10\n",
    "\n",
    "chosen = list(NN_POLICIES.keys())[3]\n",
    "network = NN_POLICIES[chosen].net\n",
    "\n",
    "for _ in range(200):\n",
    "    agents = INITIALISATION(n_agents)\n",
    "    merged_reps = represent_poolers(list(agents.values()), None, True)\n",
    "    with torch.no_grad():\n",
    "        input_tens = torch.tensor(merged_reps, dtype=torch.float32).unsqueeze(0)\n",
    "        projected = network.lin1(input_tens[0])\n",
    "    \n",
    "    for rep, agent in zip(projected, agents.values()):\n",
    "        rows.append(rep.numpy())\n",
    "        for a_name, a_index in attr_name_to_index.items():\n",
    "            attr_vals[a_name].append(agent[a_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rows)\n",
    "pca = decomposition.PCA(n_components=5)\n",
    "transformed = pca.fit_transform(df)\n",
    "[round(x*100, 1) for x in pca.explained_variance_ratio_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for attr_name, attr in attr_vals.items():   \n",
    "    plt.scatter(transformed[:,0], transformed[:,1], c=attr, cmap='coolwarm')\n",
    "    plt.colorbar().set_label(attr_name)\n",
    "    print(chosen)\n",
    "    plt.xlabel('Component 1')\n",
    "    plt.ylabel('Component 2')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idempotent = {}\n",
    "xs = np.linspace(0,1,1000)\n",
    "for x in xs:\n",
    "    example = {i: (x, *const) for i in range(10)}\n",
    "    for name, policy in [\n",
    "        ['Imitation NN - Avg', NN_POLICIES['Imitation NN - Avg']],\n",
    "        ['Reinforcement Learned (ours)', NN_POLICIES['RL 2M']], \n",
    "        ['Avg', average], \n",
    "        ['LogOp $w_i=w_{truth}$', prodop], \n",
    "    ]:\n",
    "        if name not in idempotent:\n",
    "            idempotent[name] = []\n",
    "        is_nn = True if ('Learned' in name or 'NN' in name) else False\n",
    "        merged_reps = represent_poolers(list(example.values()), None, is_nn)\n",
    "        out = policy(merged_reps)\n",
    "        idempotent[name].append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, vals in idempotent.items():\n",
    "    plt.plot(xs, vals, label=name)\n",
    "plt.legend()\n",
    "plt.ylabel('Post-pooling belief')\n",
    "plt.xlabel('Pre-pooling belief');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indiv_bayes = {}\n",
    "xs = np.linspace(0,1,100)\n",
    "for w_doubt in xs:\n",
    "    for name, policy in [\n",
    "        ['Imitation NN - Avg', NN_POLICIES['Imitation NN - Avg']],\n",
    "        ['Reinforcement Learned (ours)', NN_POLICIES['RL 2M']], \n",
    "        ['Avg', average], \n",
    "        ['LogOp $w_i=w_{truth}$', prodop], \n",
    "    ]:\n",
    "        befores = []\n",
    "        afters = []\n",
    "        for _ in range(100):\n",
    "            example = INITIALISATION(10)\n",
    "            example = {i: (b[0], 1, 1, 1, 0, w_doubt) for i, b in example.items()}\n",
    "\n",
    "            if name not in indiv_bayes:\n",
    "                indiv_bayes[name] = []\n",
    "            is_nn = True if ('Learned' in name or 'NN' in name) else False\n",
    "            \n",
    "            # if pool second\n",
    "            from swarmi.game import receive_evidence\n",
    "            new_agents = {i: [receive_evidence(a, 1) or a[0], *a[1:]] for i, a in example.items()}\n",
    "            merged_reps = represent_poolers(list(new_agents.values()), None, is_nn)\n",
    "            before_out = policy(merged_reps)\n",
    "            befores.append(float(before_out))\n",
    "            \n",
    "            # if pool second\n",
    "            merged_reps = represent_poolers(list(new_agents.values()), None, is_nn)\n",
    "            before_out = policy(merged_reps)\n",
    "            new_agents = {i: [receive_evidence([before_out, *a[1:]], 1) or before_out, *a[1:]] for i, a in example.items()}\n",
    "            after_out = sum([a[0] for a in new_agents.values()])/10\n",
    "            afters.append(float(after_out))\n",
    "\n",
    "        indiv_bayes[name].append(sum([abs(x-y) for x,y in zip(afters, befores)])/len(afters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, vals in indiv_bayes.items():\n",
    "    plt.scatter(xs, vals, label=name)\n",
    "plt.legend()\n",
    "plt.ylabel('$|x_{belief}^{pool.first} - x_{belief}^{pool.second}|$')\n",
    "plt.xlabel('$w_{doubt}$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_to_tag = {\n",
    "    'Random': 'fromrandom',\n",
    "    'Weighted Average': 'fromavg',\n",
    "    'NormLogOp': 'fromnormlogop',\n",
    "    # 'Average - $\\sum$ Agg': 'fromsumaggavg',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {}\n",
    "for name, tag in name_to_tag.items():\n",
    "    \n",
    "    with open(f\"./plots/{tag}_100k_rewards_0.txt\", 'r') as f:\n",
    "        rewards = eval(f.read())\n",
    "    \n",
    "    chkpoints = [x for x in os.listdir('./models') if tag in x and 'done' not in x]    \n",
    "    intervals = [int(x.split(f'{tag}_0_')[-1].replace('.pt', '')) for x in chkpoints]\n",
    "    intervals = sorted(intervals)\n",
    "    max_ = intervals[-1]+(intervals[-1]-intervals[-2])\n",
    "    \n",
    "    losses[name] = {'reward': rewards, 'num_episodes': np.linspace(0, max_, len(rewards)).tolist()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "for name, vals in losses.items():\n",
    "    plt.plot([x/1_000_000 for x in vals['num_episodes']], vals['reward'], label=name, marker='o', markersize=6)\n",
    "plt.xlabel('# RL Episodes (millions)', fontsize=16)\n",
    "plt.ylabel('Training Reward', fontsize=16)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.legend(fontsize=15)\n",
    "plt.xlim(0, 5.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
